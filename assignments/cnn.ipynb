{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Assignment 1\n",
    "\n",
    "#### Part II: Convolutional Neural Networks\n",
    "\n",
    "***\n",
    "\n",
    "Please see the description of the assignment in the README file (section 2) <br>\n",
    "**Guide notebook**: [material/cnns_pytorch.ipynb](material/cnns_pytorch.ipynb)\n",
    "\n",
    "Table of contents:\n",
    "1. Activate GPU\n",
    "2. Load data\n",
    "3. Inspect data\n",
    "4. Convolutional neural network (**Where you will implement the CNN**)\n",
    "5. Training hyperparameters (**Where you will add training parameters**)\n",
    "6. Training\n",
    "7. Plot loss and accuracy\n",
    "8. Evaluate\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision  # noqa\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # noqa\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a bit of a hack in case your IDE wants to run the notebook from /`assignment/` and not the project root folder `/ma1`. We need the working directory to be `/ma1` for local imports to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Git\\AIML25\\mas\\ma1\n"
     ]
    }
   ],
   "source": [
    "# Ensure the working directory is set to the \"ma1\" folder.\n",
    "while Path.cwd().name != \"ma1\" and \"ma1\" in str(Path.cwd()):\n",
    "    os.chdir(\"..\")  # Move up one directory\n",
    "print(f\"Working directory set to: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are local files imported into this notebook. This is one of the advantages of not using an online notebook like Google Colab or stand-alone notebooks like Jupyter: We can import local files and use them in our code, thus making it easier to manage our code and create a more modular structure.\n",
    "\n",
    "In `/src`, we have the following files:\n",
    "- `utils.py`: Contains utility functions (e.g., setting our device to GPU if available)\n",
    "- `data.py`: Contains functions to load data, train/validation split, and data augmentation\n",
    "- `training.py`: Contains a single but very important function: Our training loop.\n",
    "- `evaluation.py`: Contains functions to evaluate our model and produce a classification report\n",
    "- `visualization.py`: Contains functions to visualize our data and model performance\n",
    "\n",
    "You are encouraged to look at these files to understand how they work. Particularly, the training and evaluation files are important to understand how we train our model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local files\n",
    "from src.utils import get_device\n",
    "from src.data import load_torch_data, to_dataloader, train_val_split\n",
    "from src.training import fit\n",
    "from src.evaluation import evaluate\n",
    "from src.visualize import plot_training_history, plot_probabilities, show_cifar_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activate GPU\n",
    "If available. Note that this is not necessary, but it will speed up your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pytorch version (2.5.1) with backend = cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()  # will default to 'cpu' if gpu is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ba54aec5f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # number of samples per batch\n",
    "\n",
    "torch.manual_seed(42)  # Set a random seed to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:10<00:00, 16.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'len(train_val)=50000, len(test)=10000'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use transforms.compose method to reformat images for modeling,\n",
    "# and save to variable preprocessing_stepss for later use.\n",
    "# Think of this like sci-kit learn's pipeline method.\n",
    "preprocessing_steps = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32,32)),             # Cifar-10 images are 32x32\n",
    "        transforms.RandomCrop(32, padding=4),   # Data augmentation step: Randomly crop the image to 32x32\n",
    "        transforms.RandomHorizontalFlip(),      # Data augmentation step: Randomly flip image horizontally\n",
    "        transforms.ToTensor(),                  # Convert the image to a pytorch tensor\n",
    "        transforms.Normalize(                   # Normalize the image (i.e. scale the image to have a mean of 0 and a standard deviation of 1)\n",
    "            mean=[0.4914, 0.4822, 0.4465],      # FYI: The mean of the CIFAR10 dataset for your convenience\n",
    "            std=[0.2023, 0.1994, 0.2010]        # FYI the standard deviation of the CIFAR10 dataset\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the training/validation data\n",
    "train_val = load_torch_data(\n",
    "    dataset=\"CIFAR10\",\n",
    "    root = 'data',                     # The root directory where the dataset will be stored\n",
    "    download = True,                   # If the dataset is not found at root, it will be downloaded\n",
    "    train = True,                      # The train dataset (as opposed to the test dataset)\n",
    "    transform = preprocessing_steps  # transformations to be applied to the dataset (see steps above)\n",
    ")\n",
    "\n",
    "# load the testing data\n",
    "test = load_torch_data(\n",
    "    dataset = \"CIFAR10\",\n",
    "    root = 'data',\n",
    "    download = True,\n",
    "    train = False,\n",
    "    transform = preprocessing_steps\n",
    ")\n",
    "\n",
    "f\"{len(train_val)=}, {len(test)=}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(val)=10000, len(train)=40000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training data in training and validation (just like train_test_split in sklearn)\n",
    "train, val = train_val_split(train_val, val_ratio=0.2, seed=42)\n",
    "\n",
    "f\"{len(val)=}, {len(train)=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloders for easy batch loading during training\n",
    "train_loader = to_dataloader(train, batch_size = batch_size, shuffle = True)\n",
    "val_loader = to_dataloader(val, batch_size = batch_size, shuffle = False)\n",
    "test_loader = to_dataloader(test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                RandomCrop(size=(32, 32), padding=4)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])\n",
       "            ),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                RandomCrop(size=(32, 32), padding=4)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])\n",
       "            ))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Pick a random example from the training set\n",
    "classes = train.dataset.classes\n",
    "selection = random.randrange(len(train)-1)\n",
    "image, label = train[selection]\n",
    "\n",
    "show_cifar_img(image, label, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        ######################\n",
    "        # Convolutional layers\n",
    "\n",
    "        # (pattern augmentation)\n",
    "        ######################\n",
    "\n",
    "        # Conv2D is a convolutional layer that applies a 2D convolution over an input signal.\n",
    "        # TODO: set the in_channels, out_channels, kernel_size, stride, and padding parameters\n",
    "        self.conv1 = nn.Conv2d(3,32 , kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
    "\n",
    "\n",
    "        # TODO: add more layers such as BatchNorm2d, MaxPool2d, and Conv2d\n",
    "\n",
    "        ######################\n",
    "        # Fully connected layers\n",
    "        # (pattern recognition)\n",
    "        ######################\n",
    "\n",
    "        # \"Flatten\" converts a 2D matrix to a vector to be able to feed it to the fully\n",
    "        # connected layers for classification. Should be between the convolutional and linear\n",
    "        # layers (as the convolutional layers output 3D tensors, and the linear layers expect 1D vectors)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Linear is a fully connected layer that applies a linear transformation to the incoming data\n",
    "        # TODO: set the in_features and out_features parameters\n",
    "        self.fc1 = nn.Linear(64*8*8, 1028)\n",
    "        self.fc2 = nn.Linear(1028, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # TODO: add more layers such as ReLU, Dropout, and Linear\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ######################\n",
    "        \n",
    "        # Convolutional forward pass\n",
    "        ######################\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        # TODO: use the layers defined in the __init__ method to build the forward pass\n",
    "\n",
    "        x = self.flatten(x) # flatten the tensor (convert 2D matrix to a vector)\n",
    "\n",
    "        ######################\n",
    "        # Fully connected forward pass\n",
    "        ######################\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # TODO: use the layers defined in the __init__ method to build the forward pass\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "LEARNING_RATE = 1e-2 # TODO: Set the learning rate\n",
    "WEIGHT_DECAY = 1e-4 # TODO: Set the weight decay (i.e. L2 regularization)\n",
    "NUM_EPOCHS = 20 # TODO: Set the number of epochs to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device) # FYI; Sends the model to the GPU if set earlier\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x14400 and 4096x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't flatten input tensors to 1D before passing to model. Convolutonal layers expect nD.\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Git\\AIML25\\mas\\ma1\\src\\training.py:79\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, val_loader, device, optimizer, criterion, num_epochs, flatten)\u001b[0m\n\u001b[0;32m     76\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Forward pass: Compute model outputs and loss\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Backward pass: Compute gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\aiml25-ma1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\aiml25-ma1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[24], line 62\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x) \u001b[38;5;66;03m# flatten the tensor (convert 2D matrix to a vector)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m######################\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Fully connected forward pass\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m######################\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\aiml25-ma1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\aiml25-ma1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\aiml25-ma1\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x14400 and 4096x1024)"
     ]
    }
   ],
   "source": [
    "model, history = fit(\n",
    "    model = model,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    device = device,\n",
    "    flatten = False  # don't flatten input tensors to 1D before passing to model. Convolutonal layers expect nD.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on training data, validation data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = train_loader,  # evaluate on training data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False  # don't flatten input tensors to 1D. Convolutonal layers expect nD.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single image and its label from the test dataset\n",
    "index = np.random.randint(len(test))  # Randomly select an index\n",
    "image, label = test[index]\n",
    "\n",
    "# Prepare the image for prediction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    input_image = image.unsqueeze(0).to(device)                     # Add batch dimension and move to device\n",
    "    outputs = model(input_image)                                    # Get model predictions\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)  # Apply softmax to get probabilities\n",
    "\n",
    "plot_probabilities(\n",
    "    image = image,\n",
    "    label = label,\n",
    "    probabilities = probabilities, \n",
    "    classes = test.classes,\n",
    "    n = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = val_loader,  # evaluate on validation data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = test_loader,  # evaluate on testing data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
